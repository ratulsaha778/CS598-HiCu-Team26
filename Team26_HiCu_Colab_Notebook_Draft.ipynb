{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blackhat-93/HiCu-Reproduce/blob/dryrun_colab/Colab_Cuong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfhrkPWr-hVE"
      },
      "source": [
        "# Introduction\n",
        "The project, titled **“HiCu-ICD”, is based on the MLHC 2022 paper \"HiCu: Leveraging Hierarchy for Curriculum Learning in Automated ICD Coding\"**. HiCu, or Hierarchical Curriculum Learning, improves ICD coding accuracy by leveraging the hierarchy of ICD codes, which groups diagnosis codes based on various organ systems in the human body.\n",
        "\n",
        "Proceedings of Machine Learning Research 182:1–25, 2022\n",
        "HiCu: Leveraging hierarchy for curriculum learning in automated ICD coding.\n",
        "W Ren, R Zeng, T Wu, T Zhu, RG Krishnan\n",
        "Machine Learning for Healthcare Conference, 198-223\n",
        "Google Scholar Link: here\n",
        "\n",
        "Weiming Ren wren@cs.toronto.edu, Ruijing Zeng jackzeng@cs.toronto.edu, Tongzi Wu tongziwu@cs.toronto.edu, Tianshu Zhu tianshu@cs.toronto.edu, Rahul G. Krishnan rahulgk@cs.toronto.edu (Department of Computer Science, University of Toronto & the Vector Institute, Toronto, Ontario, Canada)\n",
        "\n",
        "Original Code repository: https://github.com/wren93/HiCu-ICD.\n",
        "\n",
        "Improving clinician throughput is an important technological opportunity for supporting improved healthcare services. When clinicians write notes, a smart process would be beneficial which can document correct diagnosis codes against the human notes. Mapping the long and detailed clinical notes and discharge summaries buried under patient profiles (Electronic Health Records / EHR) to specific ICD (International Classification of Diseases) coding - is a time-consuming, error-prone, and challenging task due to many possible codes and the complex relationships between them. The original paper aims to address the problem of automated coding of medical diagnoses and procedures using the International Classification of Diseases (ICD) system. The paper proposes a novel hierarchical curriculum learning approach that leverages the hierarchical structure of the ICD codes to improve the accuracy and efficiency of automated ICD coding.\n",
        "\n",
        "The paper uses a hierarchical structure of ICD codes to train the model in a curriculum learning framework. The approach involves learning simpler codes before more complex codes, designed based on the hierarchical structure of the ICD codes. It takes advantage of the hierarchical structure of the ICD codes to improve the model's ability to learn complex relationships between the codes, which is difficult to achieve using traditional flat learning approaches.\n",
        "\n",
        "Curriculum learning is the design of curricula i.e., in the sequential design of tasks that gradually increase in difficulty. HiCu is an innovation over this process that can predict ICD codes from the natural language descriptions of the patients. It leverages the hierarchy of ICD codes which is grouped based on various organ systems of the human body. The HiCu algorithm uses the graph structure in the space of outputs to design curricula for multi-label classification.\n",
        "\n",
        "The algorithm is based on:\n",
        "\n",
        "*   The Tree structure: The decision boundaries for different ICD codes are not independent. The ICD codes are organized in a tree structure which defines a notion of similarity between codes. This means that dissimilar labels will have different ancestors in the tree and vice versa. As we go deeper into each sub-tree, the specificity of the codes increases. This means that HiCu can provide wider and non-overlapping decision boundaries for multi-label classification as the diseases and organs are grouped under defined boundaries.\n",
        "*   HiCu explicitly incorporates techniques to handle label imbalance. This is essential to ensure parity of performance of predictive models on both rare and frequent labels. HiCu can predict rare and frequent labels with equal accuracy.\n",
        "\n",
        "HiCu is an improvement over curriculum learning which is used in medical code prediction using graph structure for solving multi-label classification problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility\n",
        "ICD codes follow a logical grouping by following a hierarchy of disease, symptoms and body parts. The grouping helps form an ordered tree structure. The HiCu algorithm is inspired from this graph structure which is used to design curricula for multi-label classification. The hierarchical curricula learning claims to provide wider and non-overlapping decision boundary for multi-label classification as the disease and organs are grouped under defined boundaries. ICD codes are organized in a tree structure which establishes similarity, in other words dissimilar labels will have different ancestors in the tree.\n",
        "\n",
        "This is an important distinction from the curricula learning by using NLP, where the learning algorithm does not assume any relationship.\n",
        "\n",
        "As addressed in the original paper, following claims would be validated:\n",
        "\n",
        "*   When there is no hierarchy or fewer than 5 ICD Codes hierarchy, the model should perform poorly than original paper. This validates the importance of the hierarchical structure in improving the multi-label classification performance of the HiCu algorithm.\n",
        "*   Reducing the ICD Code Hierarchy should affect the model training time. This is because the hierarchical curriculum learning approach requires the model to learn from more examples in the early stages of training, which can be computationally expensive. Reducing the hierarchy may cost to have different classification performance.\n",
        "\n",
        "As part of the draft, the model was trained on MultiResCNN over ‘train_full.csv’ data with embed file ‘processed_full_100.embed’, Asymmetric loss function, and ‘HierarchicalHyperbolic’ decoder.\n",
        "\n",
        "The final project report will focus on ablation studies and highlights in original project claims.\n"
      ],
      "metadata": {
        "id": "B2Bg6kHf-fAQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "This project aims at improving the ICD classification process. It leverages a hierarchical structure for curriculum learning to improve the accuracy and efficiency of automated ICD coding. It uses the MIMIC-III dataset for model training and evaluation.\n",
        "\n",
        "The primary objective of this project is to design curricula for multi-label classification models that predict ICD diagnosis and procedure codes from natural language descriptions of patients. The project uses the MIMIC-III dataset for model training and evaluation. The data preprocessing code from MultiResCNN is used to set up the dataset. The project proposes Hierarchical Curriculum Learning (HiCu), an algorithm that uses graph structure in the space of outputs to design curricula for multi-label classification.\n"
      ],
      "metadata": {
        "id": "AMafjAer-4_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data\n",
        "The dataset MIMIC-III v1.4 (mimic 3) was downloaded from https://physionet.org/content/mimiciii/1.4/ which was first published 2nd Sept 2016 with the intent of enhancing data quality and providing a large amount of additional data for Metavision patients.\n",
        "\n",
        "Resource citations:\n",
        "Johnson, A., Pollard, T., & Mark, R. (2016). MIMIC-III Clinical Database (version 1.4). PhysioNet. https://doi.org/10.13026/C2XW26.\n",
        "\n",
        "Original publication:\n",
        "Johnson, A. E. W., Pollard, T. J., Shen, L., Lehman, L. H., Feng, M., Ghassemi, M., Moody, B., Szolovits, P., Celi, L. A., & Mark, R. G. (2016). MIMIC-III, a freely accessible critical care database. Scientific Data, 3, 160035.\n",
        "\n",
        "Citation for PhysioNet:\n",
        "Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P. C., Mark, R., ... & Stanley, H. E. (2000). PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals. Circulation [Online]. 101 (23), pp. e215–e220.\n",
        "\n",
        "The dataset is downloaded from the protected website after the necessary training was obtained and the data usage agreement was signed.\n",
        "\n",
        "This dataset provides a collection of comma-separated value (CSV) files. Each data file has its own identifiers, suffixed with ‘ID’. E.g., SUBJECT_ID is assigned to a unique patient, HADM_ID refers to a unique admission, ICUSTAY_ID relates to a unique visit to ICU. Events such as notes, laboratory tests, and fluid balance are stored in a series of ‘events’ data files. E.g., OUTPUTEVENTS contains all measurements related to output for a given patient, while LABEVENTS contains laboratory test results for a patient. Data files prefixed with ‘D_’ are dictionaries and contain definitions for identifiers. Rows in CHARTEVENTS linked to a single ITEMID represent the measured concept, but actual name of the measurement is not present in this file. When CHARTEVENTS and D_ITEMS are joined by ITEMID, the details emerge.\n"
      ],
      "metadata": {
        "id": "Fm-JHIP8_CBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data files used are placed under ‘data’ folder in project root folder in Google Drive.\n",
        "\n",
        "*data*\n",
        "*   D_ICD_DIAGNOSES.csv\n",
        "*   D_ICD_PROCEDURES.csv\n",
        "*   mimic3/PROCEDURES_ICD.csv\n",
        "*   mimic3/DIAGNOSES_ICD.csv\n",
        "*   mimic3/NOTEEVENTS.csv *italicized text*\n",
        "\n",
        "HADM files were downloaded from https://github.com/jamesmullenbach/caml-mimic/tree/master/mimicdata/mimic3. They are also loaded into Google Drive under mimic3 folder.\n",
        "\n",
        "*   dev_50_hadm_ids.csv\n",
        "*   dev_full_hadm_ids.csv\n",
        "*   test_50_hadm_ids.csv\n",
        "*   test_full_hadm_ids.csv\n",
        "*   train_50_hadm_ids.csv\n",
        "*   train_full_hadm_ids.csv\n",
        "\n"
      ],
      "metadata": {
        "id": "x9wEDnU2_sT3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some data statistics:\n",
        "*   50K+ patients, including their clinical notes and their corresponding ICD codes.\n",
        "*   52,722 summaries.\n",
        "*   8929 unique codes.\n",
        "*   The experiment used top 50 codes with a subset of 11,317 summaries for training, validation and testing.\n",
        "*   The full dataset has 47,719 training summaries, 1,631 validation summaries and 3,372 testing summaries.\n",
        "\n",
        "HiCu algorithm has 2 processes.\n",
        "*   Pre-processing: raw files are read, and intermediate files are created.\n",
        "*   Post-processing: the model is trained on the intermediate files.\n",
        "\n",
        "All these data files were preprocessed through a python script ‘preprocess_mimic3.py’.\n",
        "The script is executed by running the below command:\n",
        "\n",
        "`python preprocess_mimic3.py`\n",
        "\n",
        "Pre-processing step needed many Python libraries: genism, nltk, numpy, pandas, scikit_learn, scipy, torch, tqdm, transformers, utils.\n",
        "The inputs csv files were read from the ‘data’ folder and ‘mimic3’ sub-folder under ‘data’, under the project root. The output files were saved under ‘mimic3’ sub-folder.\n"
      ],
      "metadata": {
        "id": "LeySOj_pASSW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "The Curriculum Learning algorithm is described below.\n",
        "\n",
        "\n",
        "\n",
        "The original paper uses the Poincare model using hyperbolic embeddings. Below is the high-level encoder decoder architecture and the hierarchical curriculum learning algorithm of the ICD coding model.\n",
        "\n",
        "The HiCu architecture consists of an encoder-decoder framework combined with a hierarchical curriculum learning algorithm. The numbers in the above figure indicate the sequential execution order of our training algorithm. The model is first trained on labels at the first level of the label tree using level one decoder, and then proceeds to level two using the knowledge transfer mechanism. This process is repeated until the model reaches the final level in the label tree.\n",
        "\n",
        "During training at each level, the hyperbolic embeddings of the ICD codes are used to guide the attention computation in the decoder. The hyperbolic embeddings allow the model to learn a representation of the ICD code hierarchy that is more structured and aligned with the hierarchical structure of the codes. The model was run the original HiCu algorithm with high-order grouping of ICD code blocks to create a two-level hierarchy and is trained on the mimic3/train_full.csv dataset to verify its performance."
      ],
      "metadata": {
        "id": "JJe6t81qAu3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://github.com/ratulsaha778/CS598-HiCu-Team26/blob/main/HiCuAlgo.jpg'>\n",
        "<figcaption>HiCu Algorithm</figcaption></center>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "UQ1qiTFtN9iL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "The MultiResCNN model was first trained locally on a computer with 16GB of RAM and 128MB of GPU memory. Additionally the data was loaded into Google Drive and model training was done on the cloud using this Google Colab notebook “Team26_Colab_Notebook_1.ipynb”.\n",
        "\n",
        "The training program is run through script ‘main.py’. Model hyperparameters were maintained in ‘options.py’.\n",
        "* Depth: 5\n",
        "* Epochs: 2, 3, 5, 10, 500\n",
        "* Model: MultiResCNN\n",
        "* Decoder: Hierarchical Hyperbolic\n",
        "* Batch Size: 8, 16\n",
        "* Workers: 1, 8, 16\n",
        "* Drop Out: 0.2\n",
        "* Loss Function: ASL (Asymmetric Loss)\n"
      ],
      "metadata": {
        "id": "XHs0KeY2CrAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "The new model performance was compared with the original hierarchical model build on CAML, DR-CAML, HyperCore etc.\n",
        "\n",
        "For the draft version, the MultiResCNN model was used. Comparison of the results were done respectively on full-code and on top-50-code from MIMIC-III data files. The HiCu algorithm was also run on multiple different encoder architectures.\n"
      ],
      "metadata": {
        "id": "AWmpSGJSCwEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to mount Google drive:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# To Copy Google Drive to Google Colab local drive:\n",
        "!cp -r '/content/drive/MyDrive/HiCu-ICD-Team26-Spring24' '/content/'\n",
        "\n",
        "# Model training:\n",
        "# Different training scripts were run with the desired model configuration\n",
        "!python /content/HiCu-ICD-Team26-Spring24/runs/run_multirescnn_50.py\n",
        "\n"
      ],
      "metadata": {
        "id": "CJ-rYa59DFba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample output is shown below:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ee_3MeBZDV5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "As per the research paper, the HiCu model and its implementation on different encoders showed model performance improvements. AUC, F1 and Precion@K attributes were compared between performance outcomes from various runs on different models.\n",
        "It was observed, when the model was run deep (higher depth) into the hierarchy, and the epoch count increased, it resulted in lesser loss function.\n",
        "\n",
        "A sample output of the model performance:\n",
        "```\n",
        "epoch finish in 189.40s, loss: 0.1643\n",
        "file for evaluation: ./data/mimic3/dev_50.csv\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4113, 0.6294, 0.5163, 0.5673, 0.8816\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4590, 0.7112, 0.5641, 0.6292, 0.9161\n",
        "rec_at_5: 0.5904\n",
        "prec_at_5: 0.6037\n",
        "rec_at_8: 0.7229\n",
        "prec_at_8: 0.4864\n",
        "rec_at_15: 0.8768\n",
        "prec_at_15: 0.3299\n",
        "```\n",
        "\n",
        "**Analyses**\n",
        "\n",
        "Due to large quantity of data and low availability of higher-power GPUs, model was run with a single GPU, and only for few epochs. The team’s intention would be to run all the encoder variations.\n",
        "\n",
        "**Plans**\n",
        "\n",
        "For the final project, the project team’s plan is to extensively run the training scripts with different encoders with higher depth and higher epochs for broader period of time to get more comparable data points.\n",
        "\n",
        "Below are the training scripts that are planned to be executed and compared against the original performance numbers.\n",
        "\n",
        "•\t**MultiResCNN**:\n",
        "\n",
        "o\tMultiResCNN_50, MultiResCNN_full\n",
        "\n",
        "o\tMultiResCNN_HiCu0_full\n",
        "\n",
        "o\tMultiResCNN_HiCuA_50, MultiResCNN_HiCuA_full\n",
        "\n",
        "o\tMultiResCNN_HiCuA_asl_50, MultiResCNN_HiCuA_asl_full\n",
        "\n",
        "o\tMultiResCNN_HiCuC_50, MultiResCNN_HiCuC_full\n",
        "\n",
        "o\tMultiResCNN_HiCuC_asl_50, MultiResCNN_HiCuC_asl_full\n",
        "\n",
        "•\t**RAC**:\n",
        "\n",
        "o\tRAC_50, RAC_full\n",
        "\n",
        "o\tRAC_HiCuA_50, RAC_HiCuA_full\n",
        "\n",
        "o\tRAC_HiCuC_50, RAC_HiCuC_full\n",
        "\n",
        "•\t**LAAT**:\n",
        "\n",
        "o\tLAAT_50, LAAT_full\n",
        "\n",
        "Additionally, the model would be tested with code changes for ablation.\n"
      ],
      "metadata": {
        "id": "Kaj0fZGgDcDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "•\thttps://physionet.org/content/mimiciii/1.4/\n",
        "\n",
        "•\thttps://github.com/wren93/HiCu-ICD/blob/main/README.md\n",
        "\n",
        "•\thttps://github.com/gkajale2/HiCu-ICD-main\n",
        "\n",
        "•\thttps://github.com/blackhat-93/HiCu-Reproduce\n",
        "\n",
        "•\thttps://github.com/foxlf823/Multi-Filter-Residual-Convolutional-Neural-Network\n",
        "\n",
        "•\thttps://paperswithcode.com/paper/hicu-leveraging-hierarchy-for-curriculum\n",
        "\n",
        "•\thttps://arxiv.org/abs/2208.02301\n",
        "\n",
        "•\thttps://proceedings.mlr.press/v182/ren22a/ren22a.pdf\n",
        "\n",
        "•\thttps://github.com/jamesmullenbach/caml-mimic/tree/master/mimicdata/mimic3\n",
        "\n",
        "•\thttps://github.com/dbiswas0605/MCS-DS-CS598-DLH-HiCu\n"
      ],
      "metadata": {
        "id": "rpxvAkx_EcNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "9CtH4qcEL_lp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# -------- Executable code starts from here --------"
      ],
      "metadata": {
        "id": "4Sc7fdJqEzmH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGGdfXIhcZGW"
      },
      "source": [
        "# Pip libraries setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICVPVRnucgcI"
      },
      "outputs": [],
      "source": [
        "# # No need to run this cell\n",
        "# # Saved here as a record of package versions that worked off-the-shelf from Colab on April 2024\n",
        "\n",
        "# # Colab's python version was 3.10.12\n",
        "# !pip install gensim==4.3.2\n",
        "# !pip install nltk==3.8.1\n",
        "# !pip install numpy==1.25.2\n",
        "# !pip install pandas==2.0.3\n",
        "# !pip install scikit-learn==1.2.2\n",
        "# !pip install scipy==1.11.4\n",
        "# !pip install tqdm==4.66.2\n",
        "# !pip install transformers==4.38.2\n",
        "# !pip install packaging==24.0\n",
        "# !pip install torch==2.2.1+cu121"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb1UQ7fKm_cH"
      },
      "source": [
        "# Check package versions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSWRu3-YlTEF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b60569e6-c719-4284-ac7e-bec4f1b0d31a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "gensim: 4.3.2\n",
            "nltk: 3.8.1\n",
            "numpy: 1.25.2\n",
            "pandas: 2.0.3\n",
            "scikit-learn: 1.2.2\n",
            "scipy: 1.11.4\n",
            "tqdm: 4.66.2\n",
            "transformers: 4.38.2\n",
            "packaging: 24.0\n",
            "torch: 2.2.1+cu121\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import gensim\n",
        "import nltk\n",
        "import numpy\n",
        "import pandas\n",
        "import sklearn\n",
        "import scipy\n",
        "import tqdm\n",
        "import transformers\n",
        "import packaging\n",
        "import torch\n",
        "\n",
        "print(\"python:\", sys.version)\n",
        "print(\"gensim:\", gensim.__version__)\n",
        "print(\"nltk:\", nltk.__version__)\n",
        "print(\"numpy:\", numpy.__version__)\n",
        "print(\"pandas:\", pandas.__version__)\n",
        "print(\"scikit-learn:\", sklearn.__version__)\n",
        "print(\"scipy:\", scipy.__version__)\n",
        "print(\"tqdm:\", tqdm.__version__)\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"packaging:\", packaging.__version__)\n",
        "print(\"torch:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KonzJON3oKd0"
      },
      "source": [
        "# Check CUDA & RAM availability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDVq9m_tnIzN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1429102-6f76-4472-9df7-a3c9d3b3faad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is not available.\n",
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available. GPU: \" + torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"CUDA is not available.\")\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0AbBrCG16vJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0084db19-313b-4792-ca91-fdce7e3ae8ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5StNPQCxmFtY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b9005a9-c4ab-4e33-8d85-1ee57aff6590"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of CPU cores: 2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "num_cores = os.cpu_count()\n",
        "print(\"Number of CPU cores:\", num_cores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqGnPqDoUHCz"
      },
      "source": [
        "# Transfer data to Google Colab local drive (faster training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DG-V0JQ9mN9M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73c77c9f-ad99-40de-ecb3-c4ef51228530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Current directory: /content/HiCu-ICD-Team26-Spring24\n",
            "Content of current directory: ['Copy of HiCu.ipynb', 'Team26_Colab_Notebook_1.ipynb', 'preprocess_mimic3.py', 'requirements.txt', 'README.md', 'utils', 'runs', 'data', '.ipynb_checkpoints', 'main.py']\n"
          ]
        }
      ],
      "source": [
        "# Give access to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Copy Google Drive to Google Colab local drive\n",
        "!cp -r '/content/drive/MyDrive/HiCu-ICD-Team26-Spring24' '/content/'\n",
        "\n",
        "# Change directory to HiCu-Reproduce folder in Google Colab\n",
        "import os\n",
        "os.chdir('/content/HiCu-ICD-Team26-Spring24')\n",
        "print(\"Current directory:\", os.getcwd())\n",
        "print(\"Content of current directory:\", os.listdir())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/HiCu-ICD-Team26-Spring24')\n",
        "print(\"Current directory:\", os.getcwd())\n",
        "print(\"Content of current directory:\", os.listdir())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcmvqQ6ZCmq7",
        "outputId": "e58b0f2e-d8bf-4cc1-968d-8ae8c7f9cf7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current directory: /content/HiCu-ICD-Team26-Spring24\n",
            "Content of current directory: ['requirements.txt', 'Copy of HiCu.ipynb', 'preprocess_mimic3.py', 'utils', 'runs', 'README.md', 'db_main.py', 'main.py', 'Team26_Colab_Notebook_0.ipynb', 'main1.py', '.ipynb_checkpoints', 'data', 'Team26_Colab_Notebook_1.ipynb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLbLcyM5moKq"
      },
      "source": [
        "# Run!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZT8EWYfmU-p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ef93218-05b7-4e0a-ecef-a3a59ce4148b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current directory: /content/HiCu-ICD-Team26-Spring24\n",
            "Starting run No. 1 of 1\n",
            "Namespace(MODEL_DIR='./models', DATA_DIR='./data', MIMIC_3_DIR='./data/mimic3', MIMIC_2_DIR='./data/mimic2', data_path='./data/mimic3/train_50.csv', vocab='./data/mimic3/vocab.csv', Y='50', version='mimic3', MAX_LENGTH=4096, model='MultiResCNN', decoder='RandomlyInitialized', filter_size='3,5,9,15,19,25', num_filter_maps=50, conv_layer=1, embed_file='./data/mimic3/processed_full_100.embed', hyperbolic_dim=50, test_model=None, use_ext_emb=False, cat_hyperbolic=False, loss='BCE', asl_config='0,0,0', asl_reduction='sum', n_epochs='2,2,3,5,50', depth=5, dropout=0.2, patience=10, batch_size=8, lr=5e-05, weight_decay=0, criterion='prec_at_8', gpu='-1', num_workers=8, tune_wordemb=True, random_seed=0, thres=0.5, longformer_dir='', reader_conv_num=2, reader_trans_num=4, trans_ff_dim=1024, num_code_title_tokens=36, code_title_filter_size=9, lstm_hidden_dim=512, attn_dim=512, scheduler=0.9, scheduler_patience=5, command='python main.py --model MultiResCNN --vocab ./data/mimic3/vocab.csv --decoder RandomlyInitialized --Y 50 --data_path ./data/mimic3/train_50.csv --MAX_LENGTH 4096 --embed_file ./data/mimic3/processed_full_100.embed --tune_wordemb --batch_size 8 --lr 5e-5 --n_epochs 2,2,3,5,50 --criterion prec_at_8 --random_seed 0 --num_workers 8', gpu_list=[-1])\n",
            "loading lookups...\n",
            "Depth 0: 14\n",
            "Depth 1: 30\n",
            "Depth 2: 39\n",
            "Depth 3: 47\n",
            "Depth 4: 50\n",
            "loading pretrained embeddings from ./data/mimic3/processed_full_100.embed\n",
            "adding unk embedding\n",
            "MultiResCNN(\n",
            "  (word_rep): WordRep(\n",
            "    (embed): Embedding(51921, 100, padding_idx=0)\n",
            "    (embed_drop): Dropout(p=0.2, inplace=False)\n",
            "  )\n",
            "  (conv): ModuleList(\n",
            "    (0): ModuleList(\n",
            "      (0): Conv1d(100, 100, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (1): ResidualBlock(\n",
            "        (left): Sequential(\n",
            "          (0): Conv1d(100, 50, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
            "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): Tanh()\n",
            "          (3): Conv1d(50, 50, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
            "          (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (shortcut): Sequential(\n",
            "          (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (1): ModuleList(\n",
            "      (0): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      (1): ResidualBlock(\n",
            "        (left): Sequential(\n",
            "          (0): Conv1d(100, 50, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
            "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): Tanh()\n",
            "          (3): Conv1d(50, 50, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
            "          (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (shortcut): Sequential(\n",
            "          (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (2): ModuleList(\n",
            "      (0): Conv1d(100, 100, kernel_size=(9,), stride=(1,), padding=(4,))\n",
            "      (1): ResidualBlock(\n",
            "        (left): Sequential(\n",
            "          (0): Conv1d(100, 50, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
            "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): Tanh()\n",
            "          (3): Conv1d(50, 50, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
            "          (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (shortcut): Sequential(\n",
            "          (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (3): ModuleList(\n",
            "      (0): Conv1d(100, 100, kernel_size=(15,), stride=(1,), padding=(7,))\n",
            "      (1): ResidualBlock(\n",
            "        (left): Sequential(\n",
            "          (0): Conv1d(100, 50, kernel_size=(15,), stride=(1,), padding=(7,), bias=False)\n",
            "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): Tanh()\n",
            "          (3): Conv1d(50, 50, kernel_size=(15,), stride=(1,), padding=(7,), bias=False)\n",
            "          (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (shortcut): Sequential(\n",
            "          (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (4): ModuleList(\n",
            "      (0): Conv1d(100, 100, kernel_size=(19,), stride=(1,), padding=(9,))\n",
            "      (1): ResidualBlock(\n",
            "        (left): Sequential(\n",
            "          (0): Conv1d(100, 50, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n",
            "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): Tanh()\n",
            "          (3): Conv1d(50, 50, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n",
            "          (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (shortcut): Sequential(\n",
            "          (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (5): ModuleList(\n",
            "      (0): Conv1d(100, 100, kernel_size=(25,), stride=(1,), padding=(12,))\n",
            "      (1): ResidualBlock(\n",
            "        (left): Sequential(\n",
            "          (0): Conv1d(100, 50, kernel_size=(25,), stride=(1,), padding=(12,), bias=False)\n",
            "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): Tanh()\n",
            "          (3): Conv1d(50, 50, kernel_size=(25,), stride=(1,), padding=(12,), bias=False)\n",
            "          (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (shortcut): Sequential(\n",
            "          (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): RandomlyInitializedDecoder(\n",
            "    (U): Linear(in_features=300, out_features=50, bias=True)\n",
            "    (final): Linear(in_features=300, out_features=50, bias=True)\n",
            "    (loss_function): BCEWithLogitsLoss()\n",
            "  )\n",
            ")\n",
            "train_instances 8066\n",
            "dev_instances 1573\n",
            "test_instances 1729\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Total epochs at each level: [2, 2, 3, 5, 50]\n",
            "Training model at depth 4:\n",
            "EPOCH 0\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "epoch finish in 4937.82s, loss: 0.3305\n",
            "file for evaluation: ./data/mimic3/dev_50.csv\n",
            "\n",
            "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
            "0.0791, 0.2174, 0.0946, 0.1318, 0.7155\n",
            "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
            "0.1438, 0.6436, 0.1562, 0.2514, 0.7640\n",
            "rec_at_5: 0.3373\n",
            "prec_at_5: 0.3807\n",
            "rec_at_8: 0.4422\n",
            "prec_at_8: 0.3170\n",
            "rec_at_15: 0.6166\n",
            "prec_at_15: 0.2362\n",
            "\n",
            "evaluation finish in 216.65s\n",
            "saved metrics, params, model to directory ./models/MultiResCNN_RandomlyInitialized_A_BCE_50_Apr_13_04_39_00\n",
            "\n",
            "EPOCH 1\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "epoch finish in 4905.53s, loss: 0.2737\n",
            "file for evaluation: ./data/mimic3/dev_50.csv\n",
            "\n",
            "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
            "0.1592, 0.3589, 0.1886, 0.2472, 0.7842\n",
            "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
            "0.2510, 0.7022, 0.2809, 0.4013, 0.8328\n",
            "rec_at_5: 0.4392\n",
            "prec_at_5: 0.4745\n",
            "rec_at_8: 0.5580\n",
            "prec_at_8: 0.3893\n",
            "rec_at_15: 0.7301\n",
            "prec_at_15: 0.2763\n",
            "\n",
            "evaluation finish in 211.44s\n",
            "saved metrics, params, model to directory ./models/MultiResCNN_RandomlyInitialized_A_BCE_50_Apr_13_04_39_00\n",
            "\n",
            "EPOCH 2\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        }
      ],
      "source": [
        "# NOTE: Change the name of the .py training script to your desired model configuration\n",
        "print(\"Current directory:\", os.getcwd())\n",
        "!python /content/HiCu-ICD-Team26-Spring24/runs/run_multirescnn_50.py\n",
        "\n",
        "# Save results back to google drive\n",
        "#!cp -r '/content/HiCu-ICD-Team26-Spring24/models' '/content/drive/MyDrive/HiCu-ICD-Team26-Spring24'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}